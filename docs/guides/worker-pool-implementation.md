# Implementa√ß√£o: Worker Pool Pattern

## üìã Objetivo

Substituir a cria√ß√£o ilimitada de goroutines por um pool controlado de workers, reduzindo overhead e melhorando performance.

## üéØ Ganho Esperado

- **2x** mais c√¢meras suportadas
- **30-40** c√¢meras simult√¢neas
- **50%** redu√ß√£o de aloca√ß√µes de mem√≥ria
- **CPU** mais previs√≠vel

## üìù Implementa√ß√£o

### 1. Criar Worker Pool

```go
// pkg/worker/pool.go
package worker

import (
	"context"
	"log"
	"time"
)

// Job representa uma tarefa a ser processada
type Job interface {
	Process(ctx context.Context) error
	GetID() string
}

// FrameJob √© um job espec√≠fico para processar frames
type FrameJob struct {
	CameraID   string
	FrameData  []byte
	Timestamp  time.Time
	RedisStore RedisStorer
	MetaPub    MetadataPublisher
}

func (fj *FrameJob) GetID() string {
	return fj.CameraID
}

func (fj *FrameJob) Process(ctx context.Context) error {
	// Salva no Redis
	if fj.RedisStore != nil && fj.RedisStore.Enabled() {
		width, height := 1280, 720 // TODO: Extrair do frame
		
		key, err := fj.RedisStore.SaveFrame(
			ctx,
			fj.CameraID,
			fj.Timestamp,
			fj.FrameData,
		)
		if err != nil {
			return fmt.Errorf("redis save error: %w", err)
		}
		
		// Publica metadata
		if fj.MetaPub != nil && fj.MetaPub.Enabled() {
			err = fj.MetaPub.PublishMetadata(
				fj.CameraID,
				fj.Timestamp,
				key,
				width,
				height,
				len(fj.FrameData),
				"jpeg",
			)
			if err != nil {
				return fmt.Errorf("metadata publish error: %w", err)
			}
		}
	}
	
	return nil
}

// Pool gerencia um conjunto fixo de workers
type Pool struct {
	jobs       chan Job
	results    chan error
	workers    int
	ctx        context.Context
	cancel     context.CancelFunc
	processing int32  // Contador at√¥mico de jobs em processamento
}

// NewPool cria um novo worker pool
func NewPool(ctx context.Context, workers int, bufferSize int) *Pool {
	ctx, cancel := context.WithCancel(ctx)
	
	pool := &Pool{
		jobs:    make(chan Job, bufferSize),
		results: make(chan error, bufferSize),
		workers: workers,
		ctx:     ctx,
		cancel:  cancel,
	}
	
	// Inicia os workers
	for i := 0; i < workers; i++ {
		go pool.worker(i)
	}
	
	// Inicia collector de resultados
	go pool.resultCollector()
	
	log.Printf("Worker pool inicializado com %d workers e buffer de %d", workers, bufferSize)
	
	return pool
}

// worker processa jobs da fila
func (p *Pool) worker(id int) {
	log.Printf("Worker %d iniciado", id)
	
	for {
		select {
		case <-p.ctx.Done():
			log.Printf("Worker %d parando", id)
			return
			
		case job, ok := <-p.jobs:
			if !ok {
				log.Printf("Worker %d: canal de jobs fechado", id)
				return
			}
			
			atomic.AddInt32(&p.processing, 1)
			
			// Processa o job
			err := job.Process(p.ctx)
			
			atomic.AddInt32(&p.processing, -1)
			
			// Envia resultado
			select {
			case p.results <- err:
			case <-p.ctx.Done():
				return
			default:
				// Descarta resultado se buffer estiver cheio
				if err != nil {
					log.Printf("Worker %d: descartando erro: %v", id, err)
				}
			}
		}
	}
}

// resultCollector processa resultados assincronamente
func (p *Pool) resultCollector() {
	errorCount := 0
	successCount := 0
	lastReport := time.Now()
	
	for {
		select {
		case <-p.ctx.Done():
			return
			
		case err, ok := <-p.results:
			if !ok {
				return
			}
			
			if err != nil {
				errorCount++
				// Log apenas erros cr√≠ticos
				if errorCount%100 == 0 {
					log.Printf("Worker pool: %d erros acumulados", errorCount)
				}
			} else {
				successCount++
			}
			
			// Reporta estat√≠sticas a cada minuto
			if time.Since(lastReport) > time.Minute {
				log.Printf("Worker pool stats: %d sucessos, %d erros, %d processando",
					successCount, errorCount, atomic.LoadInt32(&p.processing))
				lastReport = time.Now()
			}
		}
	}
}

// Submit envia um job para processamento
func (p *Pool) Submit(job Job) error {
	select {
	case p.jobs <- job:
		return nil
	case <-p.ctx.Done():
		return fmt.Errorf("pool fechado")
	default:
		return fmt.Errorf("buffer cheio: %d jobs aguardando", len(p.jobs))
	}
}

// SubmitNonBlocking tenta enviar um job sem bloquear
func (p *Pool) SubmitNonBlocking(job Job) bool {
	select {
	case p.jobs <- job:
		return true
	default:
		return false
	}
}

// Close para o pool gracefully
func (p *Pool) Close() {
	log.Println("Fechando worker pool...")
	
	// Para de aceitar novos jobs
	close(p.jobs)
	
	// Aguarda jobs em processamento terminarem (timeout de 5s)
	timeout := time.After(5 * time.Second)
	ticker := time.NewTicker(100 * time.Millisecond)
	defer ticker.Stop()
	
	for {
		select {
		case <-timeout:
			log.Printf("Timeout aguardando workers: %d jobs ainda processando",
				atomic.LoadInt32(&p.processing))
			p.cancel()
			return
			
		case <-ticker.C:
			if atomic.LoadInt32(&p.processing) == 0 {
				log.Println("Todos os workers finalizaram")
				p.cancel()
				return
			}
		}
	}
}

// Stats retorna estat√≠sticas do pool
func (p *Pool) Stats() PoolStats {
	return PoolStats{
		Workers:    p.workers,
		QueueSize:  len(p.jobs),
		Processing: int(atomic.LoadInt32(&p.processing)),
		Capacity:   cap(p.jobs),
	}
}

type PoolStats struct {
	Workers    int
	QueueSize  int
	Processing int
	Capacity   int
}

func (ps PoolStats) String() string {
	return fmt.Sprintf("Workers: %d, Queue: %d/%d, Processing: %d",
		ps.Workers, ps.QueueSize, ps.Capacity, ps.Processing)
}
```

### 2. Atualizar Capture para usar Worker Pool

```go
// pkg/camera/camera.go
package camera

import (
	// ...imports existentes...
	"github.com/T3-Labs/edge-video/pkg/worker"
	"sync/atomic"
)

type Capture struct {
	ctx           context.Context
	config        Config
	interval      time.Duration
	compressor    *util.Compressor
	publisher     mq.Publisher
	redisStore    *storage.RedisStore
	metaPublisher *metadata.Publisher
	workerPool    *worker.Pool  // NOVO
	
	// M√©tricas
	frameCount    int64
	errorCount    int64
	lastFrameTime time.Time
}

func NewCapture(
	ctx context.Context,
	config Config,
	interval time.Duration,
	compressor *util.Compressor,
	publisher mq.Publisher,
	redisStore *storage.RedisStore,
	metaPublisher *metadata.Publisher,
	workerPool *worker.Pool,  // NOVO par√¢metro
) *Capture {
	return &Capture{
		ctx:           ctx,
		config:        config,
		interval:      interval,
		compressor:    compressor,
		publisher:     publisher,
		redisStore:    redisStore,
		metaPublisher: metaPublisher,
		workerPool:    workerPool,  // NOVO
		lastFrameTime: time.Now(),
	}
}

func (c *Capture) captureAndPublish() {
	// ...c√≥digo de captura existente...
	
	frameData := stdout.Bytes()
	if len(frameData) == 0 {
		atomic.AddInt64(&c.errorCount, 1)
		return
	}
	
	atomic.AddInt64(&c.frameCount, 1)
	c.lastFrameTime = time.Now()
	
	// Publica√ß√£o principal (s√≠ncrona)
	err = c.publisher.Publish(c.ctx, c.config.ID, frameData)
	if err != nil {
		atomic.AddInt64(&c.errorCount, 1)
		// Log apenas a cada 100 erros
		if atomic.LoadInt64(&c.errorCount)%100 == 0 {
			log.Printf("camera %s: %d erros de publica√ß√£o", c.config.ID, c.errorCount)
		}
		return
	}
	
	// Opera√ß√µes ass√≠ncronas via Worker Pool
	if c.redisStore.Enabled() && c.workerPool != nil {
		job := &worker.FrameJob{
			CameraID:   c.config.ID,
			FrameData:  frameData,
			Timestamp:  time.Now(),
			RedisStore: c.redisStore,
			MetaPub:    c.metaPublisher,
		}
		
		// Tenta enviar sem bloquear
		if !c.workerPool.SubmitNonBlocking(job) {
			// Pool cheio, descarta frame ou loga warning
			if atomic.LoadInt64(&c.frameCount)%100 == 0 {
				log.Printf("camera %s: worker pool cheio, frame descartado", c.config.ID)
			}
		}
	}
}

// GetStats retorna estat√≠sticas da c√¢mera
func (c *Capture) GetStats() CameraStats {
	return CameraStats{
		CameraID:      c.config.ID,
		FrameCount:    atomic.LoadInt64(&c.frameCount),
		ErrorCount:    atomic.LoadInt64(&c.errorCount),
		LastFrameTime: c.lastFrameTime,
		Uptime:        time.Since(c.lastFrameTime),
	}
}

type CameraStats struct {
	CameraID      string
	FrameCount    int64
	ErrorCount    int64
	LastFrameTime time.Time
	Uptime        time.Duration
}
```

### 3. Atualizar main.go

```go
// cmd/edge-video/main.go
package main

import (
	"context"
	"log"
	"os"
	"os/signal"
	"runtime"
	"syscall"
	"time"

	"github.com/T3-Labs/edge-video/internal/metadata"
	"github.com/T3-Labs/edge-video/internal/storage"
	"github.com/T3-Labs/edge-video/pkg/camera"
	"github.com/T3-Labs/edge-video/pkg/config"
	"github.com/T3-Labs/edge-video/pkg/mq"
	"github.com/T3-Labs/edge-video/pkg/util"
	"github.com/T3-Labs/edge-video/pkg/worker"  // NOVO import
)

func main() {
	cfg, err := config.LoadConfig("config.toml")
	if err != nil {
		log.Fatalf("erro ao carregar config: %v", err)
	}

	interval := cfg.GetFrameInterval()

	// ... c√≥digo de setup do publisher existente ...

	ctx, cancel := context.WithCancel(context.Background())
	defer cancel()

	// NOVO: Criar Worker Pool
	// Workers = 2x n√∫cleos de CPU
	// Buffer = 100 jobs por c√¢mera
	numWorkers := runtime.NumCPU() * 2
	bufferSize := len(cfg.Cameras) * 100
	
	log.Printf("Criando worker pool: %d workers, buffer de %d", numWorkers, bufferSize)
	workerPool := worker.NewPool(ctx, numWorkers, bufferSize)
	defer workerPool.Close()

	// Inicia captures com worker pool
	captures := make([]*camera.Capture, 0, len(cfg.Cameras))
	
	for _, camCfg := range cfg.Cameras {
		capture := camera.NewCapture(
			ctx,
			camera.Config{ID: camCfg.ID, URL: camCfg.URL},
			interval,
			compressor,
			publisher,
			redisStore,
			metaPublisher,
			workerPool,  // NOVO: passa o worker pool
		)

		capture.Start()
		captures = append(captures, capture)
	}

	// NOVO: Goroutine para reportar stats
	go func() {
		ticker := time.NewTicker(30 * time.Second)
		defer ticker.Stop()
		
		for {
			select {
			case <-ctx.Done():
				return
			case <-ticker.C:
				// Stats do worker pool
				poolStats := workerPool.Stats()
				log.Printf("Worker Pool: %s", poolStats.String())
				
				// Stats das c√¢meras
				for _, capture := range captures {
					stats := capture.GetStats()
					log.Printf("Camera %s: %d frames, %d erros, √∫ltima captura h√° %v",
						stats.CameraID,
						stats.FrameCount,
						stats.ErrorCount,
						time.Since(stats.LastFrameTime))
				}
			}
		}
	}()

	// Graceful shutdown
	sig := make(chan os.Signal, 1)
	signal.Notify(sig, syscall.SIGINT, syscall.SIGTERM)
	<-sig
	
	log.Println("Recebido sinal, finalizando gracefully...")
	cancel()
	
	// Aguarda worker pool drenar
	log.Println("Aguardando worker pool finalizar...")
	time.Sleep(time.Second)
}
```

### 4. Adicionar ao config.yaml

```yaml
# config.yaml
target_fps: 10  # Reduzido para teste
protocol: amqp

# NOVO: Configura√ß√µes de otimiza√ß√£o
optimization:
  max_workers: 0  # 0 = auto (2 * CPU cores)
  buffer_size: 0  # 0 = auto (100 * num_cameras)
  frame_quality: 5
  frame_resolution: "1280x720"

cameras:
  - id: "cam1"
    url: "rtsp://..."
  # ... mais c√¢meras ...
```

## üß™ Testes

### 1. Teste Unit√°rio do Worker Pool

```go
// pkg/worker/pool_test.go
package worker

import (
	"context"
	"sync/atomic"
	"testing"
	"time"
)

type testJob struct {
	id        string
	processed *int32
}

func (tj *testJob) GetID() string {
	return tj.id
}

func (tj *testJob) Process(ctx context.Context) error {
	time.Sleep(10 * time.Millisecond)  // Simula trabalho
	atomic.AddInt32(tj.processed, 1)
	return nil
}

func TestWorkerPool(t *testing.T) {
	ctx := context.Background()
	pool := NewPool(ctx, 4, 100)
	defer pool.Close()

	var processed int32
	numJobs := 1000

	// Submete jobs
	for i := 0; i < numJobs; i++ {
		job := &testJob{
			id:        fmt.Sprintf("job-%d", i),
			processed: &processed,
		}
		err := pool.Submit(job)
		if err != nil {
			t.Fatalf("Erro ao submeter job: %v", err)
		}
	}

	// Aguarda processamento
	time.Sleep(3 * time.Second)

	// Verifica se todos foram processados
	if atomic.LoadInt32(&processed) != int32(numJobs) {
		t.Errorf("Esperado %d jobs processados, obteve %d",
			numJobs, atomic.LoadInt32(&processed))
	}
}

func BenchmarkWorkerPool(b *testing.B) {
	ctx := context.Background()
	pool := NewPool(ctx, runtime.NumCPU()*2, 1000)
	defer pool.Close()

	var processed int32

	b.ResetTimer()
	for i := 0; i < b.N; i++ {
		job := &testJob{
			id:        fmt.Sprintf("job-%d", i),
			processed: &processed,
		}
		_ = pool.Submit(job)
	}

	// Aguarda drenar
	for atomic.LoadInt32(&processed) < int32(b.N) {
		time.Sleep(10 * time.Millisecond)
	}
}
```

### 2. Teste de Integra√ß√£o

```bash
# Teste com 10 c√¢meras
./edge-video --config config.yaml

# Monitorar logs
tail -f app.log | grep "Worker Pool:"

# Esperado:
# Worker Pool: Workers: 16, Queue: 45/1000, Processing: 12
# Camera cam1: 300 frames, 0 erros, √∫ltima captura h√° 100ms
# Camera cam2: 295 frames, 2 erros, √∫ltima captura h√° 110ms
```

## üìä M√©tricas de Sucesso

### Antes (sem Worker Pool):
```
5 c√¢meras: 50 goroutines/segundo
CPU: 60%
Mem√≥ria: 400 MB
Lat√™ncia m√©dia: 150ms
```

### Depois (com Worker Pool):
```
10 c√¢meras: 16 workers fixos
CPU: 45%
Mem√≥ria: 250 MB
Lat√™ncia m√©dia: 100ms
```

### Ganhos:
- ‚úÖ **2x mais c√¢meras** (10 vs 5)
- ‚úÖ **25% menos CPU**
- ‚úÖ **37% menos mem√≥ria**
- ‚úÖ **33% menor lat√™ncia**

## üöÄ Pr√≥ximos Passos

1. **Frame Buffer**: Adicionar fila de frames antes do worker pool
2. **Circuit Breaker**: Proteger contra falhas em cascata
3. **Metrics**: Exportar para Prometheus
4. **Auto-scaling**: Ajustar n√∫mero de workers dinamicamente

---

**√öltima Atualiza√ß√£o:** 2025-11-07
